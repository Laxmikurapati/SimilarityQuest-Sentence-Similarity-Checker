{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/laxmikurapati/quora-question-similarity-with-bert?scriptVersionId=138401292\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas\n\nquestions_dataset = pandas.read_csv(\"/kaggle/input/datacsvp/train.csv\", index_col=\"id\", nrows=3000)\nquestions_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:28.759712Z","iopub.execute_input":"2023-06-30T16:36:28.761174Z","iopub.status.idle":"2023-06-30T16:36:28.807321Z","shell.execute_reply.started":"2023-06-30T16:36:28.761123Z","shell.execute_reply":"2023-06-30T16:36:28.806137Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"    qid1  qid2                                          question1  \\\nid                                                                  \n0      1     2  What is the step by step guide to invest in sh...   \n1      3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n2      5     6  How can I increase the speed of my internet co...   \n3      7     8  Why am I mentally very lonely? How can I solve...   \n4      9    10  Which one dissolve in water quikly sugar, salt...   \n\n                                            question2  is_duplicate  \nid                                                                   \n0   What is the step by step guide to invest in sh...             0  \n1   What would happen if the Indian government sto...             0  \n2   How can Internet speed be increased by hacking...             0  \n3   Find the remainder when [math]23^{24}[/math] i...             0  \n4             Which fish would survive in salt water?             0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid1</th>\n      <th>qid2</th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>2</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>What is the step by step guide to invest in sh...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>4</td>\n      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n      <td>What would happen if the Indian government sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>6</td>\n      <td>How can I increase the speed of my internet co...</td>\n      <td>How can Internet speed be increased by hacking...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7</td>\n      <td>8</td>\n      <td>Why am I mentally very lonely? How can I solve...</td>\n      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>9</td>\n      <td>10</td>\n      <td>Which one dissolve in water quikly sugar, salt...</td>\n      <td>Which fish would survive in salt water?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's see what we got:","metadata":{}},{"cell_type":"code","source":"questions_dataset.info()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:28.809904Z","iopub.execute_input":"2023-06-30T16:36:28.810821Z","iopub.status.idle":"2023-06-30T16:36:28.841532Z","shell.execute_reply.started":"2023-06-30T16:36:28.810777Z","shell.execute_reply":"2023-06-30T16:36:28.840049Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 3000 entries, 0 to 2999\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   qid1          3000 non-null   int64 \n 1   qid2          3000 non-null   int64 \n 2   question1     3000 non-null   object\n 3   question2     3000 non-null   object\n 4   is_duplicate  3000 non-null   int64 \ndtypes: int64(3), object(2)\nmemory usage: 140.6+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import BertTokenizerFast\n\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:28.84381Z","iopub.execute_input":"2023-06-30T16:36:28.844861Z","iopub.status.idle":"2023-06-30T16:36:32.95076Z","shell.execute_reply.started":"2023-06-30T16:36:28.844813Z","shell.execute_reply":"2023-06-30T16:36:32.94936Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b97564ca2f194c8bacb65002573bc1eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83678baab784ba39b6e64e89d86009a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e51d7583ff72495f8016fcba2081ebca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d37087a7575948aa9a3b7364bdbda1a9"}},"metadata":{}}]},{"cell_type":"markdown","source":"Split a sentence into tokens:","metadata":{}},{"cell_type":"code","source":"print(f\"Original: {questions_dataset['question1'][0]}\")\nprint(f\"Tokenized: {tokenizer.tokenize(questions_dataset['question1'][0])}\")\nprint(f\"Token IDs: {tokenizer.convert_tokens_to_ids(tokenizer.tokenize(questions_dataset['question1'][0]))}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:32.952815Z","iopub.execute_input":"2023-06-30T16:36:32.954118Z","iopub.status.idle":"2023-06-30T16:36:32.970113Z","shell.execute_reply.started":"2023-06-30T16:36:32.954065Z","shell.execute_reply":"2023-06-30T16:36:32.968747Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Original: What is the step by step guide to invest in share market in india?\nTokenized: ['what', 'is', 'the', 'step', 'by', 'step', 'guide', 'to', 'invest', 'in', 'share', 'market', 'in', 'india', '?']\nToken IDs: [2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1999, 2634, 1029]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can combine ``tokenize`` and ``convert_tokens_to_ids`` with the method ``encode``:","metadata":{}},{"cell_type":"code","source":"print(f\"Original: {questions_dataset['question2'][0]}\")\nprint(f\"Tokenized: {tokenizer.encode(questions_dataset['question2'][0])}\")","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:32.973318Z","iopub.execute_input":"2023-06-30T16:36:32.973652Z","iopub.status.idle":"2023-06-30T16:36:32.994583Z","shell.execute_reply.started":"2023-06-30T16:36:32.973624Z","shell.execute_reply":"2023-06-30T16:36:32.99353Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Original: What is the step by step guide to invest in share market?\nTokenized: [101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1029, 102]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see we got two extra tokens ``101`` and ``102``. Let's see what they are:","metadata":{}},{"cell_type":"code","source":"tokenizer.decode([101, 2054, 2003, 1996, 3357, 2011, 3357, 5009, 2000, 15697, 1999, 3745, 3006, 1029, 102])","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:32.995761Z","iopub.execute_input":"2023-06-30T16:36:32.996562Z","iopub.status.idle":"2023-06-30T16:36:45.812238Z","shell.execute_reply.started":"2023-06-30T16:36:32.99653Z","shell.execute_reply":"2023-06-30T16:36:45.810996Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'[CLS] what is the step by step guide to invest in share market? [SEP]'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Special Tokens\n``[SEP]`` - At the end of every sentence, we need to append the special ``[SEP]`` token.\n\nThis token is an artifact of two-sentence tasks, where BERT is given two separate sentences and asked to determine something (remeber BERT was trained intially to perdict question-answering task).\n\n``[CLS]`` - For classification tasks, we must prepend the special ``[CLS]`` token to the beginning of every sentence.\n\nThis token has special significance. BERT consists of 12 Transformer layers. Each transformer takes in a list of token embeddings, and produces the same number of embeddings on the output.\n\nSo now let's see how to encode the two questions together:","metadata":{}},{"cell_type":"code","source":"encoded_pair = tokenizer.encode(questions_dataset['question1'][0], questions_dataset['question2'][0])\ntokenizer.decode(encoded_pair)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:45.813466Z","iopub.execute_input":"2023-06-30T16:36:45.814203Z","iopub.status.idle":"2023-06-30T16:36:45.823445Z","shell.execute_reply.started":"2023-06-30T16:36:45.814169Z","shell.execute_reply":"2023-06-30T16:36:45.822209Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'[CLS] what is the step by step guide to invest in share market in india? [SEP] what is the step by step guide to invest in share market? [SEP]'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Sentence Length\n\nThe sentences in our dataset obviously have varying lengths, so how does BERT handle this?\n\nBERT has two constraints:\n\n1. All sentences must be padded or truncated to a single, fixed length.\n2. The maximum sentence length is 512 tokens.\nPadding is done with a special ``[PAD]`` token, which is at index 0 in the BERT vocabulary.\n\nThe maximum length does impact training and evaluation speed.\n\nBefore we are ready to encode our text, though, we need to decide on a maximum sentence length for padding / truncating to.\n\nLet's find the maximum length of question pairs:","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\n\n\n\ntqdm.pandas()\nquestions_dataset[\"question1_length\"] = questions_dataset[\"question1\"].progress_apply(lambda question: \n                                                                                      len(tokenizer.tokenize(question)))\nquestions_dataset[\"question2_length\"] = questions_dataset[\"question2\"].progress_apply(lambda question: \n                                                                                      len(tokenizer.tokenize(question)))\nquestions_dataset[\"joint_length\"] = questions_dataset[\"question1_length\"] + questions_dataset[\"question2_length\"]\nquestions_dataset[\"joint_length\"].max()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:45.825333Z","iopub.execute_input":"2023-06-30T16:36:45.826274Z","iopub.status.idle":"2023-06-30T16:36:46.542401Z","shell.execute_reply.started":"2023-06-30T16:36:45.826229Z","shell.execute_reply":"2023-06-30T16:36:46.541261Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 3000/3000 [00:00<00:00, 8928.62it/s]\n100%|██████████| 3000/3000 [00:00<00:00, 8525.90it/s]\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"141"},"metadata":{}}]},{"cell_type":"markdown","source":"We will choose max length of 310.\n\n# Training & Validation Split\n\nI'm going to spit the set into 3200 records for train (64%), 800 records for validation (16%) and 1000 records for test (20%).\n\nLet's Create train, validation and test:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(questions_dataset[[\"question1\", \"question2\"]], \n                                                    questions_dataset[\"is_duplicate\"], test_size=0.2, random_state=42)\nX_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\nX_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:46.543987Z","iopub.execute_input":"2023-06-30T16:36:46.544414Z","iopub.status.idle":"2023-06-30T16:36:47.404842Z","shell.execute_reply.started":"2023-06-30T16:36:46.544373Z","shell.execute_reply":"2023-06-30T16:36:47.403655Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                                              question1  \\\nid                                                        \n1221  Which is a suitable solar panel installation p...   \n1651  Why don't we connect the direct DC power that ...   \n1460  A couple who has 5 sons went for a picnic. Eac...   \n1081                        Did Trump win the election?   \n1405  What does the Tagalog word \"salamat\" mean in E...   \n\n                                              question2  \nid                                                       \n1221  Which is a good solar panel installation provi...  \n1651  Since India has its own one nation one grid co...  \n1460  Why are there so many \"no picnic\" signs in Ven...  \n1081  What do you think of Trump winning the elections?  \n1405     Tagalog: What does salamat po mean in English?  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1221</th>\n      <td>Which is a suitable solar panel installation p...</td>\n      <td>Which is a good solar panel installation provi...</td>\n    </tr>\n    <tr>\n      <th>1651</th>\n      <td>Why don't we connect the direct DC power that ...</td>\n      <td>Since India has its own one nation one grid co...</td>\n    </tr>\n    <tr>\n      <th>1460</th>\n      <td>A couple who has 5 sons went for a picnic. Eac...</td>\n      <td>Why are there so many \"no picnic\" signs in Ven...</td>\n    </tr>\n    <tr>\n      <th>1081</th>\n      <td>Did Trump win the election?</td>\n      <td>What do you think of Trump winning the elections?</td>\n    </tr>\n    <tr>\n      <th>1405</th>\n      <td>What does the Tagalog word \"salamat\" mean in E...</td>\n      <td>Tagalog: What does salamat po mean in English?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"y_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:47.406419Z","iopub.execute_input":"2023-06-30T16:36:47.407126Z","iopub.status.idle":"2023-06-30T16:36:47.414344Z","shell.execute_reply.started":"2023-06-30T16:36:47.406988Z","shell.execute_reply":"2023-06-30T16:36:47.413452Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"id\n1221    0\n1651    0\n1460    0\n1081    0\n1405    0\nName: is_duplicate, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"# Tokenize Dataset and Create Dataloader\n\nNow we’re ready to perform the real tokenization.\n\nThe tokenizer.encode_plus function combines multiple steps for us:\n\n1. Split the sentence into tokens.\n2. Add the special ``[CLS]`` and ``[SEP]`` tokens.\n3. Map the tokens to their IDs.\n4. Pad or truncate all sentences to the same length.\n5. Create the attention masks which explicitly differentiate real tokens from ``[PAD]`` tokens.\n\nDocumentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).","metadata":{}},{"cell_type":"code","source":"max_length = 300\ntokenizer.encode_plus(X_train.iloc[0][\"question1\"], X_train.iloc[0][\"question2\"], max_length=max_length, \n                      pad_to_max_length=True, return_attention_mask=True, return_tensors='pt', truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:47.415806Z","iopub.execute_input":"2023-06-30T16:36:47.416353Z","iopub.status.idle":"2023-06-30T16:36:47.492466Z","shell.execute_reply.started":"2023-06-30T16:36:47.416315Z","shell.execute_reply":"2023-06-30T16:36:47.491152Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2029,  2003,  1037,  7218,  5943,  5997,  8272, 10802,  2379,\n          2148,  4796,  1010,  2662,  6187,  1029,   102,  2029,  2003,  1037,\n          2204,  5943,  5997,  8272, 10802,  2379,  2358,  7076,  2239,  3509,\n          1010,  2662,  6187,  1029,   102,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"So what we got:\n\n1. input_ids - Token ids padded with 0 at the end.\n2. token_type_ids - This array indicates bert what is the first sentence and what is the seconds. In case of classification of only one sentance this array is redundant.\n3. attention_mask - The “Attention Mask” is simply an array of 1s and 0s indicating which tokens are padding and which aren’t. This mask tells the “Self-Attention” mechanism in BERT not to incorporate these ``[PAD]`` tokens into its interpretation of the sentence.\n\nAll the array are in the size of ``max_length``.\n\nNow let's create our datasets:","metadata":{}},{"cell_type":"code","source":"import torch\n\nfrom tqdm import tqdm\nfrom torch.utils.data import TensorDataset\ndef convert_to_dataset_torch(data: pandas.DataFrame, labels: pandas.Series) -> TensorDataset:\n    input_ids = []\n    attention_masks = []\n    token_type_ids = []\n    for _, row in tqdm(data.iterrows(), total=data.shape[0]):\n        encoded_dict = tokenizer.encode_plus(row[\"question1\"], row[\"question2\"], max_length=max_length, pad_to_max_length=True, \n                      return_attention_mask=True, return_tensors='pt', truncation=True)\n        # Add the encoded sentences to the list.\n        input_ids.append(encoded_dict['input_ids'])\n        token_type_ids.append(encoded_dict[\"token_type_ids\"])\n        # And its attention mask (simply differentiates padding from non-padding).\n        attention_masks.append(encoded_dict['attention_mask'])\n    \n    # Convert the lists into tensors.\n    input_ids = torch.cat(input_ids, dim=0)\n    token_type_ids = torch.cat(token_type_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n    labels = torch.tensor(labels.values)\n    \n    return TensorDataset(input_ids, attention_masks, token_type_ids, labels)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:47.494154Z","iopub.execute_input":"2023-06-30T16:36:47.494637Z","iopub.status.idle":"2023-06-30T16:36:47.505777Z","shell.execute_reply.started":"2023-06-30T16:36:47.494564Z","shell.execute_reply":"2023-06-30T16:36:47.504491Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\ntrain = convert_to_dataset_torch(X_train, y_train)\nvalidation = convert_to_dataset_torch(X_validation, y_validation)\n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:47.506898Z","iopub.execute_input":"2023-06-30T16:36:47.507809Z","iopub.status.idle":"2023-06-30T16:36:49.63037Z","shell.execute_reply.started":"2023-06-30T16:36:47.507774Z","shell.execute_reply":"2023-06-30T16:36:49.62915Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 1920/1920 [00:01<00:00, 1169.16it/s]\n100%|██████████| 480/480 [00:00<00:00, 1219.75it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We’ll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory:","metadata":{}},{"cell_type":"code","source":"import multiprocessing\n\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\n# The DataLoader needs to know our batch size for training, so we specify it \n# here.\n\nbatch_size = 20\n\ncore_number = multiprocessing.cpu_count()\n\n# Create the DataLoaders for our training and validation sets.\n# We'll take training samples in random order. \ntrain_dataloader = DataLoader(\n            train,  # The training samples.\n            sampler = RandomSampler(train), # Select batches randomly\n            batch_size = batch_size, # Trains with this batch size.\n            num_workers = core_number\n        )\n\n# For validation the order doesn't matter, so we'll just read them sequentially.\nvalidation_dataloader = DataLoader(\n            validation, # The validation samples.\n            sampler = SequentialSampler(validation), # Pull out batches sequentially.\n            batch_size = batch_size, # Evaluate with this batch size.\n            num_workers = core_number\n        )","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:49.635915Z","iopub.execute_input":"2023-06-30T16:36:49.636594Z","iopub.status.idle":"2023-06-30T16:36:49.645103Z","shell.execute_reply.started":"2023-06-30T16:36:49.636558Z","shell.execute_reply":"2023-06-30T16:36:49.643847Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Trainig the Classification Model\nNow that our input data is properly formatted, it’s time to fine tune the BERT model.\n## BertForSequenceClassification\nFor this task, we first want to modify the pre-trained BERT model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task.\n\nThankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained BERT model, each has different top layers and output types designed to accomodate their specific NLP task.\n\nWe’ll be using [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification). This is the normal BERT model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained BERT model and the additional untrained classification layer is trained on our specific task.\n\nOK, let’s load BERT! There are a few different pre-trained BERT models available. “bert-base-uncased” means the version that has only lowercase letters (“uncased”) and is the smaller version of the two (“base” vs “large”).","metadata":{}},{"cell_type":"code","source":"from transformers import BertForSequenceClassification\n\n# Load BertForSequenceClassification, the pretrained BERT model with a single \n# linear classification layer on top. \n\nbert_model = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n    num_labels=2, # The number of output labels--2 for binary classification.\n                    # You can increase this for multi-class tasks.   \n    output_attentions=False, # Whether the model returns attentions weights.\n    output_hidden_states=False, # Whether the model returns all hidden-states.\n)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:49.646799Z","iopub.execute_input":"2023-06-30T16:36:49.647166Z","iopub.status.idle":"2023-06-30T16:36:53.839489Z","shell.execute_reply.started":"2023-06-30T16:36:49.647135Z","shell.execute_reply":"2023-06-30T16:36:53.838384Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31dedf3dd5da4bb1a401f938c1e19f9e"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Optimizer & Learning Rate Scheduler\nNow that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n\nFor the purposes of fine-tuning, the authors recommend choosing from the following values (from Appendix A.3 of the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)):\n\n* Batch size: 16, 32\n* Learning rate (Adam): 5e-5, 3e-5, 2e-5\n* Number of epochs: 2, 3, 4\n\nI chose:\n* Batch size: 32\n* Learning rate: 2e-5\n* Number of epochs: 2\n\nAll due to hardware performance issues.\n\nLet's create our optimizer:","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW\n# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\nadamw_optimizer = AdamW(bert_model.parameters(),\n                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.841215Z","iopub.execute_input":"2023-06-30T16:36:53.84175Z","iopub.status.idle":"2023-06-30T16:36:53.862424Z","shell.execute_reply.started":"2023-06-30T16:36:53.841707Z","shell.execute_reply":"2023-06-30T16:36:53.861172Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”.\n\nLet's create the learning rate scheduler:","metadata":{}},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\n# Number of training epochs. The BERT authors recommend between 2 and 4. \nepochs = 1\n\n# Total number of training steps is [number of batches] x [number of epochs]. \n# (Note that this is not the same as the number of training samples).\n\ntotal_steps = len(train_dataloader) * epochs\n\n# Create the learning rate scheduler.\nscheduler = get_linear_schedule_with_warmup(adamw_optimizer, \n                                            num_warmup_steps = 0, \n                                            num_training_steps = total_steps)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.863869Z","iopub.execute_input":"2023-06-30T16:36:53.864275Z","iopub.status.idle":"2023-06-30T16:36:53.903547Z","shell.execute_reply.started":"2023-06-30T16:36:53.86424Z","shell.execute_reply":"2023-06-30T16:36:53.90233Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Training Loop\nHelper function for formatting elapsed times as ``hh:mm:ss``:","metadata":{}},{"cell_type":"code","source":"import time\nimport datetime\n\ndef format_time(elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str(datetime.timedelta(seconds=elapsed_rounded))","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.904826Z","iopub.execute_input":"2023-06-30T16:36:53.905602Z","iopub.status.idle":"2023-06-30T16:36:53.921549Z","shell.execute_reply.started":"2023-06-30T16:36:53.905566Z","shell.execute_reply":"2023-06-30T16:36:53.920534Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"This method is to train one batch over our model:","metadata":{}},{"cell_type":"code","source":"def fit_batch(dataloader, model, optimizer, epoch):\n    total_train_loss = 0\n    \n    for batch in tqdm(dataloader, desc=f\"Training epoch:{epoch}\", unit=\"batch\"):\n        # Unpack batch from dataloader.\n        input_ids, attention_masks, token_type_ids, labels = batch\n        \n        # clear any previously calculated gradients before performing a backward pass.\n        model.zero_grad()\n        \n        # Perform a forward pass (evaluate the model on this training batch).\n        outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n        loss = outputs.loss\n        total_train_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients.\n        loss.backward()\n\n        # Clip the norm of the gradients to 1.0.\n        # This is to help prevent the \"exploding gradients\" problem.\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        # Update parameters and take a step using the computed gradient.\n        optimizer.step()\n\n        # Update the learning rate.\n        scheduler.step()\n        \n    return total_train_loss","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.922944Z","iopub.execute_input":"2023-06-30T16:36:53.924574Z","iopub.status.idle":"2023-06-30T16:36:53.936971Z","shell.execute_reply.started":"2023-06-30T16:36:53.924527Z","shell.execute_reply":"2023-06-30T16:36:53.935799Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"This method is evalute one batch over our model. We're going to use scikit learn [accuracy_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html):","metadata":{}},{"cell_type":"code","source":"import numpy\n\nfrom sklearn.metrics import accuracy_score\n\ndef eval_batch(dataloader, model, metric=accuracy_score):\n    total_eval_accuracy = 0\n    total_eval_loss = 0\n    predictions , predicted_labels = [], []\n    \n    for batch in tqdm(dataloader, desc=\"Evaluating\", unit=\"batch\"):\n        # Unpack batch from dataloader.\n        input_ids, attention_masks, token_type_ids, labels = batch\n        \n        # Tell pytorch not to bother with constructing the compute graph during\n        # the forward pass, since this is only needed for backprop (training).\n        with torch.no_grad():\n            # Forward pass, calculate logit predictions.\n            outputs = model(input_ids=input_ids, attention_mask=attention_masks, labels=labels)\n            loss = outputs.loss\n            logits = outputs.logits\n        total_eval_loss += loss.item()\n        \n        # Calculate the accuracy for this batch of validation sentences, and\n        # accumulate it over all batches.\n        y_pred = numpy.argmax(logits.detach().numpy(), axis=1).flatten()\n        total_eval_accuracy += metric(labels, y_pred)\n        \n        predictions.extend(logits.detach().numpy().tolist())\n        predicted_labels.extend(y_pred.tolist())\n    \n    return total_eval_accuracy, total_eval_loss, predictions ,predicted_labels","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.93919Z","iopub.execute_input":"2023-06-30T16:36:53.939903Z","iopub.status.idle":"2023-06-30T16:36:53.951444Z","shell.execute_reply.started":"2023-06-30T16:36:53.93986Z","shell.execute_reply":"2023-06-30T16:36:53.950307Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Main loop method:","metadata":{}},{"cell_type":"code","source":"import random\n\n# Set the seed value all over the place to make this reproducible.\n\nseed_val = 10\n\nrandom.seed(seed_val)\nnumpy.random.seed(seed_val)\ntorch.manual_seed(seed_val)\n\n\ndef train(train_dataloader, validation_dataloader, model, optimizer, epochs):\n    # We'll store a number of quantities such as training and validation loss, \n    # validation accuracy, and timings.\n    training_stats = []\n    \n    # Measure the total training time for the whole run.\n    total_t0 = time.time()\n    \n    for epoch in range(0, epochs):\n        \n        # Measure how long the training epoch takes.\n        t0 = time.time()\n        \n        # Reset the total loss for this epoch.\n        total_train_loss = 0\n        \n        # Put the model into training mode. \n        model.train()\n        \n        total_train_loss = fit_batch(train_dataloader, model, optimizer, epoch)\n        \n        # Calculate the average loss over all of the batches.\n        avg_train_loss = total_train_loss / len(train_dataloader)\n        \n        # Measure how long this epoch took.\n        training_time = format_time(time.time() - t0)\n        \n        t0 = time.time()\n        \n        # Put the model in evaluation mode--the dropout layers behave differently\n        # during evaluation.\n        model.eval()\n        \n        total_eval_accuracy, total_eval_loss, _, _ = eval_batch(validation_dataloader, model)\n        \n        # Report the final accuracy for this validation run.\n        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n        \n        print(f\"  Accuracy: {avg_val_accuracy}\")\n    \n        # Calculate the average loss over all of the batches.\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n    \n        # Measure how long the validation run took.\n        validation_time = format_time(time.time() - t0)\n    \n        print(f\"  Validation Loss: {avg_val_loss}\")\n    \n        # Record all statistics from this epoch.\n        training_stats.append(\n            {\n                'epoch': epoch,\n                'Training Loss': avg_train_loss,\n                'Valid. Loss': avg_val_loss,\n                'Valid. Accur.': avg_val_accuracy,\n                'Training Time': training_time,\n                'Validation Time': validation_time\n            }\n        )\n        \n\n    print(\"\")\n    print(\"Training complete!\")\n\n    print(f\"Total training took {format_time(time.time()-total_t0)}\")\n    return training_stats","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.953072Z","iopub.execute_input":"2023-06-30T16:36:53.953488Z","iopub.status.idle":"2023-06-30T16:36:53.978083Z","shell.execute_reply.started":"2023-06-30T16:36:53.953453Z","shell.execute_reply":"2023-06-30T16:36:53.976717Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"We’re ready to kick off the training:","metadata":{}},{"cell_type":"code","source":"training_stats = train(train_dataloader, validation_dataloader, bert_model, adamw_optimizer, epochs)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T16:36:53.979851Z","iopub.execute_input":"2023-06-30T16:36:53.980925Z","iopub.status.idle":"2023-06-30T17:29:18.547769Z","shell.execute_reply.started":"2023-06-30T16:36:53.980886Z","shell.execute_reply":"2023-06-30T17:29:18.546631Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Training epoch:0:   0%|          | 0/96 [00:00<?, ?batch/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Training epoch:0: 100%|██████████| 96/96 [48:33<00:00, 30.09s/batch] ","output_type":"stream"},{"name":"stdout","text":"To disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Training epoch:0: 100%|██████████| 96/96 [48:33<00:00, 30.35s/batch]\nEvaluating:   0%|          | 0/24 [00:00<?, ?batch/s]","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 24/24 [03:50<00:00,  9.62s/batch]","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.74375\n  Validation Loss: 0.5148955409725507\n\nTraining complete!\nTotal training took 0:52:25\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let’s view the summary of the training process:","metadata":{}},{"cell_type":"code","source":"df_stats = pandas.DataFrame(training_stats).set_index('epoch')\ndf_stats","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:29:18.54966Z","iopub.execute_input":"2023-06-30T17:29:18.55006Z","iopub.status.idle":"2023-06-30T17:29:18.570102Z","shell.execute_reply.started":"2023-06-30T17:29:18.550021Z","shell.execute_reply":"2023-06-30T17:29:18.568781Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"       Training Loss  Valid. Loss  Valid. Accur. Training Time Validation Time\nepoch                                                                         \n0            0.59672     0.514896        0.74375       0:48:34         0:03:51","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Training Loss</th>\n      <th>Valid. Loss</th>\n      <th>Valid. Accur.</th>\n      <th>Training Time</th>\n      <th>Validation Time</th>\n    </tr>\n    <tr>\n      <th>epoch</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.59672</td>\n      <td>0.514896</td>\n      <td>0.74375</td>\n      <td>0:48:34</td>\n      <td>0:03:51</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from matplotlib import pyplot\n%matplotlib inline\npyplot.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\npyplot.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\npyplot.title(\"Training & Validation Loss\")\npyplot.xlabel(\"Epoch\")\npyplot.ylabel(\"Loss\")\npyplot.legend()\npyplot.xticks(df_stats.index.values.tolist())\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:29:18.571611Z","iopub.execute_input":"2023-06-30T17:29:18.572064Z","iopub.status.idle":"2023-06-30T17:29:18.90018Z","shell.execute_reply.started":"2023-06-30T17:29:18.572026Z","shell.execute_reply":"2023-06-30T17:29:18.899244Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAkAAAAHFCAYAAAAaD0bAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7RElEQVR4nO3deVxVdeL/8fflKpsCKghcZdNMVMxMGBXMLQy3Mc2cLJd00swWRzOndNz3MTdqSieb1DHXXFq+SSqWFCOaZebYNzUrBUIYU0fQTDA4vz/8cb9dWURFrnBez8fjPOJ8zud8zudw6XHffj5nsRiGYQgAAMBEXJzdAQAAgIpGAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAALKicViKdOSlJR0U8eZNm2aLBbLDe2blJRULn24GXv27FHHjh3l7e0tPz8/3Xfffdq1a1eZ9n355ZdlsVi0bdu2Euu88cYbslgs2rJlS5n71KlTJ3Xq1MmhzGKxaNq0adfcd+XKlbJYLDpx4kSZj1coISGhxGOEhYVp6NCh193mzSr8G9m0aVOFHxuoSNWc3QGgqtizZ4/D+syZM7Vr1y59/PHHDuXNmjW7qeMMHz5c3bp1u6F9W7VqpT179tx0H25UamqqunbtqoiICK1du1b5+flKTEzUF198oc6dO19z/0GDBunFF1/U8uXLS/wdrFixQnXr1lWvXr1uqq979uxRUFDQTbVxLQkJCXrttdeKDUHvvPOOvL29b+nxATMjAAHlpG3btg7rdevWlYuLS5Hyq128eFGenp5lPk5QUNANfzF7e3tfsz+3UkJCgs6fP68VK1aoSZMmkqTevXuXeX9fX1/17t1b7777rs6cOSNfX1+H7UeOHNGePXv0/PPPq3r16jfVV2f+niTpnnvucerxgaqOKTCgAnXq1EnNmzfXp59+qpiYGHl6eurxxx+XJG3YsEFxcXGy2Wzy8PBQ06ZNNX78eP38888ObRQ3BRYWFqbf//732rZtm1q1aiUPDw81adJEy5cvd6hX3BTY0KFDVbNmTX333Xfq0aOHatasqeDgYD3//PPKzc112P/HH39Uv3795OXlpVq1amngwIH6/PPPZbFYtHLlymuev9VqlSQdPXq0rL+yIoYNG6a8vDytXbu2yLYVK1ZIkv13On36dLVp00Z16tSRt7e3WrVqpTfffFNleQd0cVNge/fuVbt27eTu7q569eppwoQJunz5cpF9y/JZDh06VK+99pr9WIVL4VRacVNgaWlpGjRokPz9/eXm5qamTZtq4cKFKigosNc5ceKELBaLFixYoEWLFqlBgwaqWbOmoqOjtXfv3mued1l9/fXX6t27t2rXri13d3e1bNlS//znPx3qFBQUaNasWQoPD5eHh4dq1aqlFi1a6OWXX7bX+emnnzRixAgFBwfLzc1NdevWVbt27bRz585y6ytQHEaAgAqWmZmpQYMG6YUXXtCcOXPk4nLl3yHHjh1Tjx49NGbMGNWoUUNHjhzRvHnztG/fviLTaMU5ePCgnn/+eY0fP14BAQH6xz/+oWHDhqlRo0bq0KFDqftevnxZDzzwgIYNG6bnn39en376qWbOnCkfHx9NmTJFkvTzzz+rc+fOOnv2rObNm6dGjRpp27Zt6t+/f5nP/aGHHtKECRM0cuRIRUREqFGjRmXet1CXLl0UGhqq5cuXa9SoUfby/Px8vfXWW2rbtq19iu/EiRN68sknFRISIulKgBk1apQyMjLs51VW33zzjWJjYxUWFqaVK1fK09NTS5YsKTaIleWznDx5sn7++Wdt2rTJYfrUZrMVe/yffvpJMTExysvL08yZMxUWFqYPPvhA48aN0/fff68lS5Y41H/ttdfUpEkTxcfH24/Xo0cPHT9+XD4+Ptd17lc7evSoYmJi5O/vr1deeUW+vr5avXq1hg4dqv/85z964YUXJEkvvfSSpk2bpkmTJqlDhw66fPmyjhw5onPnztnbGjx4sL788kvNnj1bjRs31rlz5/Tll1/qzJkzN9VH4JoMALfEkCFDjBo1ajiUdezY0ZBkfPTRR6XuW1BQYFy+fNn45JNPDEnGwYMH7dumTp1qXP2/bmhoqOHu7m6kpqbay3755RejTp06xpNPPmkv27VrlyHJ2LVrl0M/JRlvv/22Q5s9evQwwsPD7euvvfaaIcn48MMPHeo9+eSThiRjxYoVpZ6TYRjG+++/bwQEBBjBwcFGcHCw8f33319zn+IU/g6+/PJLe9n//M//GJKMN954o9h98vPzjcuXLxszZswwfH19jYKCAvu2jh07Gh07dnSoL8mYOnWqfb1///6Gh4eHkZWVZS/79ddfjSZNmhiSjOPHjxd73NI+y2eeeabIZ1koNDTUGDJkiH19/PjxhiTjs88+c6j31FNPGRaLxTh69KhhGIZx/PhxQ5Jx1113Gb/++qu93r59+wxJxrp164o9XqHCv5GNGzeWWOeRRx4x3NzcjLS0NIfy7t27G56ensa5c+cMwzCM3//+90bLli1LPV7NmjWNMWPGlFoHuBWYAgMqWO3atXXfffcVKf/hhx80YMAABQYGymq1qnr16urYsaMk6fDhw9dst2XLlvaRDklyd3dX48aNlZqaes19LRZLkYuGW7Ro4bDvJ598Ii8vryIXHz/66KPXbF+SUlJS9NBDD2nJkiXavXu3qlevrs6dO+v48eP2OsOHD1doaOg12/rjH/8oFxcXhym+FStWqEaNGg4jUh9//LG6dOkiHx8f++90ypQpOnPmjE6dOlWmfhfatWuXYmNjFRAQYC+zWq3FjoDd7GdZnI8//ljNmjVT69atHcqHDh0qwzCKjBL27NnTPuUoXfk8JZXp76EsfYmNjVVwcHCRvly8eNE+otW6dWsdPHhQTz/9tLZv366cnJwibbVu3VorV67UrFmztHfv3mKnFIFbgQAEVLDipjguXLig9u3b67PPPtOsWbOUlJSkzz//3H4r9y+//HLNdq++IFiS3NzcyrSvp6en3N3di+x76dIl+/qZM2ccvvwLFVdWnNmzZys8PFx9+/ZVcHCwPvnkE3sISk1NVUFBgZKTk9WzZ89rthUaGqrY2FitXbtWubm5On36tD744AP94Q9/kJeXlyRp3759iouLk3Tl1vjdu3fr888/18SJEyWV7Xf6W2fOnFFgYGCR8qvLyuOzLOn4xf3t1KtXz779t67+e3Bzc7up499IXyZMmKAFCxZo79696t69u3x9fRUbG6svvvjCvs+GDRs0ZMgQ/eMf/1B0dLTq1Kmjxx57TFlZWTfdT6A0XAMEVLDinuHz8ccf6+TJk0pKSrKPFEhyuFbC2Xx9fbVv374i5WX9ovr+++8dvpSDgoL0ySefqFOnTurcubOGDh2q1NRUjRs3rkztDRs2TImJiXrvvfd08uRJ5eXladiwYfbt69evV/Xq1fXBBx84hLt33323TO1fzdfXt9hzvbrsVn2Wvr6+yszMLFJ+8uRJSZKfn99NtX8r+lKtWjWNHTtWY8eO1blz57Rz50795S9/UdeuXZWeni5PT0/5+fkpPj5e8fHxSktL0/vvv6/x48fr1KlTpT7vCbhZjAABt4HCUFT4r/RCr7/+ujO6U6yOHTvq/Pnz+vDDDx3K169fX6b9mzdvrv379+ubb76xl9WvX1+ffPKJDMPQ1KlTNX78eDVs2LBM7fXp00e+vr5avny5VqxYocaNG+vee++1b7dYLKpWrZrDNNAvv/yit956q0ztX61z58766KOP9J///Mdelp+frw0bNjjUu57P8npGZWJjY/XNN9/oyy+/dChftWqVLBZLmZ6jVF5iY2PtQe/qvnh6ehb7CIFatWqpX79+euaZZ3T27NliHxwZEhKiZ599Vvfff3+R8wTKGyNAwG0gJiZGtWvX1siRIzV16lRVr15da9as0cGDB53dNbshQ4Zo8eLFGjRokGbNmqVGjRrpww8/1Pbt2yXJfjdbSWbNmqWPP/5YnTp10p///Ge1atVKZ8+e1datW/Xjjz8qKChIS5cuVf/+/dW0adNr9sfNzU0DBw7U3/72NxmGob/+9a8O23v27KlFixZpwIABGjFihM6cOaMFCxYUCSZlNWnSJL3//vu67777NGXKFHl6euq1114r8piC6/ks77rrLknSvHnz1L17d1mtVrVo0UKurq5F6j733HNatWqVevbsqRkzZig0NFRbt27VkiVL9NRTT6lx48Y3dF4lKemW+Y4dO2rq1Kn64IMP1LlzZ02ZMkV16tTRmjVrtHXrVr300kv2u8x69eql5s2bKyoqSnXr1lVqaqri4+MVGhqqO++8U9nZ2ercubMGDBigJk2ayMvLS59//rm2bdumvn37luv5AEU4+SJsoMoq6S6wiIiIYuunpKQY0dHRhqenp1G3bl1j+PDhxpdfflnkDquS7gLr2bNnkTavvruppLvAru5nScdJS0sz+vbta9SsWdPw8vIyHnroISMhIcGQZLz33nsl/Srsjh8/bgwdOtSoV6+eUa1aNcPf39/4wx/+YOzZs8f4z3/+Y9xxxx1GYGCg/Y6mazl48KAhybBarcbJkyeLbF++fLkRHh5uuLm5GQ0bNjTmzp1rvPnmm0Xu2irLXWCGYRi7d+822rZta7i5uRmBgYHGn//8Z2PZsmVF2ivrZ5mbm2sMHz7cqFu3rmGxWBzaufouMMMwjNTUVGPAgAGGr6+vUb16dSM8PNyYP3++kZ+f7/A7lmTMnz+/yO+juHO6WuHfSElL4d/OoUOHjF69ehk+Pj6Gq6urcffddxe5E3DhwoVGTEyM4efnZ7i6uhohISHGsGHDjBMnThiGYRiXLl0yRo4cabRo0cLw9vY2PDw8jPDwcGPq1KnGzz//XGo/gZtlMYwyPBEMAEowZ84cTZo0SWlpabf81REAUF6YAgNQZq+++qokqUmTJrp8+bI+/vhjvfLKKxo0aBDhB0ClQgACUGaenp5avHixTpw4odzcXIWEhOjFF1/UpEmTnN01ALguTIEBAADT4TZ4AABgOgQgAABgOgQgAABgOk6/CHrJkiWaP3++MjMzFRERofj4eLVv377E+rm5uZoxY4ZWr16trKwsBQUFaeLEiXr88cftdTZv3qzJkyfr+++/1x133KHZs2frwQcfLHOfCgoKdPLkSXl5eRX72gIAAHD7MQxD58+fV7169a75cFanPghx/fr1RvXq1Y033njD+Oabb4zRo0cbNWrUMFJTU0vc54EHHjDatGljJCYmGsePHzc+++wzY/fu3fbtKSkphtVqNebMmWMcPnzYmDNnjlGtWjVj7969Ze5Xenp6qQ8CY2FhYWFhYbl9l/T09Gt+1zv1LrA2bdqoVatWWrp0qb2sadOm6tOnj+bOnVuk/rZt2/TII4/ohx9+UJ06dYpts3///srJyXF4X1G3bt1Uu3ZtrVu3rkz9ys7OVq1atZSeni5vb+/rPCsAAOAMOTk5Cg4O1rlz5+yvZCmJ06bA8vLytH//fo0fP96hPC4uTikpKcXu8/777ysqKkovvfSS3nrrLdWoUUMPPPCAZs6cKQ8PD0nSnj179Nxzzzns17VrV8XHx5fYl9zcXOXm5trXz58/L0ny9vYmAAEAUMmU5fIVpwWg06dPKz8/XwEBAQ7lAQEBysrKKnafH374Qf/617/k7u6ud955R6dPn9bTTz+ts2fPavny5ZKkrKys62pTkubOnavp06ff5BkBAIDKwul3gV2d0gzDKDG5FRQUyGKxaM2aNWrdurV69OihRYsWaeXKlfrll19uqE1JmjBhgrKzs+1Lenr6TZwRAAC43TltBMjPz09Wq7XIyMypU6eKjOAUstlsql+/vsO8XtOmTWUYhn788UfdeeedCgwMvK42JcnNzU1ubm43cTYAAKAycVoAcnV1VWRkpBITEx1uUU9MTFTv3r2L3addu3bauHGjLly4oJo1a0qSvv32W7m4uNhfxBgdHa3ExESH64B27NihmJiYW3g2AIDbVX5+vi5fvuzsbqCcuLq6XvsW9zJw6nOAxo4dq8GDBysqKkrR0dFatmyZ0tLSNHLkSElXpqYyMjK0atUqSdKAAQM0c+ZM/fGPf9T06dN1+vRp/fnPf9bjjz9uvwh69OjR6tChg+bNm6fevXvrvffe086dO/Wvf/3LaecJAKh4hmEoKytL586dc3ZXUI5cXFzUoEEDubq63lQ7Tg1A/fv315kzZzRjxgxlZmaqefPmSkhIUGhoqCQpMzNTaWlp9vo1a9ZUYmKiRo0apaioKPn6+urhhx/WrFmz7HViYmK0fv16TZo0SZMnT9Ydd9yhDRs2qE2bNhV+fgAA5ykMP/7+/vL09OTBtlVA4YOKMzMzFRISclOfKW+DL0ZOTo58fHyUnZ3NbfAAUAnl5+fr22+/lb+/v3x9fZ3dHZSj7OxsnTx5Uo0aNVL16tUdtl3P97fT7wIDAKC8FV7z4+np6eSeoLwVTn3l5+ffVDsEIABAlcW0V9VTXp+p01+GCgAVJT9fSk6WMjMlm01q316yWp3dKwDOwAgQAFPYskUKC5M6d5YGDLjy37CwK+VAVdepUyeNGTOmzPVPnDghi8Wir7766pb1ydkYAQJQ5W3ZIvXrJ119y0dGxpXyTZukvn2d0zfc/ipy5PBa0ztDhgzRypUrr7vdLVu2FLlguDTBwcHKzMyUn5/fdR+rsiAAAajS8vOl0aOLhh/pSpnFIo0ZI/XuzXQYitqy5crfz48//l9ZUJD08su3JjRnZmbaf96wYYOmTJmio0eP2ssKn3lX6PLly2UKNnXq1LmuflitVgUGBl7XPpUNU2AAqrTkZMcvr6sZhpSefqUe8FuFI4dX//0UjhzeiunTwMBA++Lj4yOLxWJfv3TpkmrVqqW3335bnTp1kru7u1avXq0zZ87o0UcfVVBQkDw9PXXXXXdp3bp1Du1ePQUWFhamOXPm6PHHH5eXl5dCQkK0bNky+/arp8CSkpJksVj00UcfKSoqSp6enoqJiXEIZ5I0a9Ys+fv7y8vLS8OHD9f48ePVsmXL8v9FlQMCEIAq7Tf/oC6Xeqi8DEP6+eeyLTk50p/+VPLIoXRlZCgnp2ztlecT91588UX96U9/0uHDh9W1a1ddunRJkZGR+uCDD/T1119rxIgRGjx4sD777LNS21m4cKGioqJ04MABPf3003rqqad05MiRUveZOHGiFi5cqC+++ELVqlXT448/bt+2Zs0azZ49W/PmzdP+/fsVEhKipUuXlss53wpMgQGo0my28q2HyuviRen/v0byphnGlZGh37ybu1QXLkg1apTPsceMGaO+V82/jRs3zv7zqFGjtG3bNm3cuLHUtyD06NFDTz/9tKQroWrx4sVKSkpSkyZNStxn9uzZ6tixoyRp/Pjx6tmzpy5duiR3d3f97W9/07Bhw/THP/5RkjRlyhTt2LFDFy5cuOFzvZUYAQJQpbVvf+WajZKuLbVYpODgK/WAyiAqKsphPT8/X7Nnz1aLFi3k6+urmjVraseOHQ6vkipOixYt7D8XTrWdOnWqzPvY/v+/Ggr3OXr0qFq3bu1Q/+r12wkjQACqNKv1ygWr/fpdCTu/nYooDEXx8VwAbQaenldGYsri00+lHj2uXS8hQerQoWzHLi81rhpKWrhwoRYvXqz4+HjdddddqlGjhsaMGaO8vLxS27n64mmLxaKCgoIy71N4x9pv97n6Lrbb+W1bjAABqPL69r1yq3v9+o7lQUHcAm8mFsuVaaiyLHFxZRs5jIsrW3u38oHUycnJ6t27twYNGqS7775bDRs21LFjx27dAUsQHh6uffv2OZR98cUXFd6PsmIECIAp9O175VZ3ngSNsqhMI4eNGjXS5s2blZKSotq1a2vRokXKyspS06ZNK7Qfo0aN0hNPPKGoqCjFxMRow4YN+ve//62GDRtWaD/KigAEwDSsVqlTJ2f3ApVF4chhcc8Bio+/fUYOJ0+erOPHj6tr167y9PTUiBEj1KdPH2VnZ1doPwYOHKgffvhB48aN06VLl/Twww9r6NChRUaFbhcW43aeoHOSnJwc+fj4KDs7W97e3s7uDgDgOl26dEnHjx9XgwYN5O7uflNt8Q65G3f//fcrMDBQb731Vrm1Wdpnez3f34wAAQBQCkYOy+bixYv6+9//rq5du8pqtWrdunXauXOnEhMTnd21YhGAAADATbNYLEpISNCsWbOUm5ur8PBwbd68WV26dHF214pFAAIAADfNw8NDO3fudHY3yozb4AEAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAgOkQgAAAqEI6deqkMWPG2NfDwsIUHx9f6j4Wi0XvvvvuTR+7vNqpCAQgAABKkV+Qr6QTSVp3aJ2STiQpvyD/lh2rV69eJT44cM+ePbJYLPryyy+vq83PP/9cI0aMKI/u2U2bNk0tW7YsUp6Zmanu3buX67FuFR6ECABACbYc3qLR20brx5z/extqkHeQXu72svo2Lf+3oQ4bNkx9+/ZVamqqQkNDHbYtX75cLVu2VKtWra6rzbp165ZnF0sVGBhYYce6WYwAAQBQjC2Ht6jf2/0cwo8kZeRkqN/b/bTl8JZyP+bvf/97+fv7a+XKlQ7lFy9e1IYNG9SnTx89+uijCgoKkqenp+666y6tW7eu1DavngI7duyYOnToIHd3dzVr1qzYd3W9+OKLaty4sTw9PdWwYUNNnjxZly9fliStXLlS06dP18GDB2WxWGSxWOz9vXoK7NChQ7rvvvvk4eEhX19fjRgxQhcuXLBvHzp0qPr06aMFCxbIZrPJ19dXzzzzjP1YtxIjQAAAUzAMQxcvXyxT3fyCfP3pwz/JkFG0HRmyyKLRH45WlwZdZHW59qvhPat7ymKxXLNetWrV9Nhjj2nlypWaMmWKfZ+NGzcqLy9Pw4cP17p16/Tiiy/K29tbW7du1eDBg9WwYUO1adPmmu0XFBSob9++8vPz0969e5WTk+NwvVAhLy8vrVy5UvXq1dOhQ4f0xBNPyMvLSy+88IL69++vr7/+Wtu2bbO/+sLHx6dIGxcvXlS3bt3Utm1bff755zp16pSGDx+uZ5991iHg7dq1SzabTbt27dJ3332n/v37q2XLlnriiSeueT43gwAEADCFi5cvqubcmuXSliFDP57/UT7zin7xF+fChAuq4VqjTHUff/xxzZ8/X0lJSercubOkK9Nfffv2Vf369TVu3Dh73VGjRmnbtm3auHFjmQLQzp07dfjwYZ04cUJBQUGSpDlz5hS5bmfSpEn2n8PCwvT8889rw4YNeuGFF+Th4aGaNWuqWrVqpU55rVmzRr/88otWrVqlGjWunPurr76qXr16ad68eQoICJAk1a5dW6+++qqsVquaNGminj176qOPPiIAAQBgJk2aNFFMTIyWL1+uzp076/vvv1dycrJ27Nih/Px8/fWvf9WGDRuUkZGh3Nxc5ebm2gPGtRw+fFghISH28CNJ0dHRRept2rRJ8fHx+u6773ThwgX9+uuv8vb2vq7zOHz4sO6++26HvrVr104FBQU6evSoPQBFRETIav2/UTSbzaZDhw5d17FuBAEIAGAKntU9dWHChWtXlPRp6qfqsbbHNeslDEhQh9AOZTr29Rg2bJieffZZvfbaa1qxYoVCQ0MVGxur+fPna/HixYqPj9ddd92lGjVqaMyYMcrLyytTu4ZRdErv6qm5vXv36pFHHtH06dPVtWtX+fj4aP369Vq4cOF1nYNhGCVO+/22vHr16kW2FRQUXNexbgQBCABgChaLpczTUHF3xCnIO0gZORnFXgdkkUVB3kGKuyOuTNcAXa+HH35Yo0eP1tq1a/XPf/5TTzzxhCwWi5KTk9W7d28NGjRI0pVreo4dO6amTZuWqd1mzZopLS1NJ0+eVL169SRdub3+t3bv3q3Q0FBNnDjRXpaamupQx9XVVfn5pT8OoFmzZvrnP/+pn3/+2T4KtHv3brm4uKhx48Zl6u+txF1gAABcxepi1cvdXpZ0Jez8VuF6fLf4WxJ+JKlmzZrq37+//vKXv+jkyZMaOnSoJKlRo0ZKTExUSkqKDh8+rCeffFJZWVllbrdLly4KDw/XY489poMHDyo5Odkh6BQeIy0tTevXr9f333+vV155Re+8845DnbCwMB0/flxfffWVTp8+rdzc3CLHGjhwoNzd3TVkyBB9/fXX2rVrl0aNGqXBgwfbp7+ciQAEAEAx+jbtq00Pb1J97/oO5UHeQdr08KZb8hyg3xo2bJj++9//qkuXLgoJCZEkTZ48Wa1atVLXrl3VqVMnBQYGqk+fPmVu08XFRe+8845yc3PVunVrDR8+XLNnz3ao07t3bz333HN69tln1bJlS6WkpGjy5MkOdR566CF169ZNnTt3Vt26dYu9Fd/T01Pbt2/X2bNn9bvf/U79+vVTbGysXn311ev/ZdwCFqO4CUGTy8nJkY+Pj7Kzs6/7oi8AgPNdunRJx48fV4MGDeTu7n5TbeUX5Cs5LVmZ5zNl87KpfUj7Wzbyg2sr7bO9nu9vrgECAKAUVherOoV1cnY3UM6YAgMAAKZDAAIAAKZDAAIAAKZDAAIAVFnc51P1lNdnSgACAFQ5hU8XvnixbC8/ReVR+NTr374+40ZwFxgAoMqxWq2qVauWTp06JenKM2nK8jZ23N4KCgr0008/ydPTU9Wq3VyEIQABAKqkwjeVF4YgVA0uLi4KCQm56UBLAAIAVEkWi0U2m03+/v66fPmys7uDcuLq6ioXl5u/gocABACo0qxW601fL4Kqh4ugAQCA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6RCAAACA6Tg9AC1ZskQNGjSQu7u7IiMjlZycXGLdpKQkWSyWIsuRI0cc6sXHxys8PFweHh4KDg7Wc889p0uXLt3qUwEAAJVENWcefMOGDRozZoyWLFmidu3a6fXXX1f37t31zTffKCQkpMT9jh49Km9vb/t63bp17T+vWbNG48eP1/LlyxUTE6Nvv/1WQ4cOlSQtXrz4lp0LAACoPJwagBYtWqRhw4Zp+PDhkq6M3Gzfvl1Lly7V3LlzS9zP399ftWrVKnbbnj171K5dOw0YMECSFBYWpkcffVT79u0r9/4DAIDKyWlTYHl5edq/f7/i4uIcyuPi4pSSklLqvvfcc49sNptiY2O1a9cuh2333nuv9u/fbw88P/zwgxISEtSzZ8/yPQEAAFBpOW0E6PTp08rPz1dAQIBDeUBAgLKysordx2azadmyZYqMjFRubq7eeustxcbGKikpSR06dJAkPfLII/rpp5907733yjAM/frrr3rqqac0fvz4EvuSm5ur3Nxc+3pOTk45nCEAALhdOXUKTJIsFovDumEYRcoKhYeHKzw83L4eHR2t9PR0LViwwB6AkpKSNHv2bC1ZskRt2rTRd999p9GjR8tms2ny5MnFtjt37lxNnz69nM4IAADc7pw2Bebn5yer1VpktOfUqVNFRoVK07ZtWx07dsy+PnnyZA0ePFjDhw/XXXfdpQcffFBz5szR3LlzVVBQUGwbEyZMUHZ2tn1JT0+/sZMCAACVgtMCkKurqyIjI5WYmOhQnpiYqJiYmDK3c+DAAdlsNvv6xYsX5eLieFpWq1WGYcgwjGLbcHNzk7e3t8MCAACqLqdOgY0dO1aDBw9WVFSUoqOjtWzZMqWlpWnkyJGSrozMZGRkaNWqVZKu3CUWFhamiIgI5eXlafXq1dq8ebM2b95sb7NXr15atGiR7rnnHvsU2OTJk/XAAw/IarU65TwBAMDtxakBqH///jpz5oxmzJihzMxMNW/eXAkJCQoNDZUkZWZmKi0tzV4/Ly9P48aNU0ZGhjw8PBQREaGtW7eqR48e9jqTJk2SxWLRpEmTlJGRobp166pXr16aPXt2hZ8fAAC4PVmMkuaFTCwnJ0c+Pj7Kzs5mOgwAgErier6/nf4qDAAAgIpGAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKZDAAIAAKbj9AC0ZMkSNWjQQO7u7oqMjFRycnKJdZOSkmSxWIosR44ccah37tw5PfPMM7LZbHJ3d1fTpk2VkJBwq08FAABUEtWcefANGzZozJgxWrJkidq1a6fXX39d3bt31zfffKOQkJAS9zt69Ki8vb3t63Xr1rX/nJeXp/vvv1/+/v7atGmTgoKClJ6eLi8vr1t6LgAAoPJwagBatGiRhg0bpuHDh0uS4uPjtX37di1dulRz584tcT9/f3/VqlWr2G3Lly/X2bNnlZKSourVq0uSQkNDy73vAACg8nLaFFheXp7279+vuLg4h/K4uDilpKSUuu8999wjm82m2NhY7dq1y2Hb+++/r+joaD3zzDMKCAhQ8+bNNWfOHOXn55f7OQAAgMrJaSNAp0+fVn5+vgICAhzKAwIClJWVVew+NptNy5YtU2RkpHJzc/XWW28pNjZWSUlJ6tChgyTphx9+0Mcff6yBAwcqISFBx44d0zPPPKNff/1VU6ZMKbbd3Nxc5ebm2tdzcnLK6SwBAMDtyKlTYJJksVgc1g3DKFJWKDw8XOHh4fb16Ohopaena8GCBfYAVFBQIH9/fy1btkxWq1WRkZE6efKk5s+fX2IAmjt3rqZPn15OZwQAAG53TpsC8/Pzk9VqLTLac+rUqSKjQqVp27atjh07Zl+32Wxq3LixrFarvaxp06bKyspSXl5esW1MmDBB2dnZ9iU9Pf06zwYAAFQmTgtArq6uioyMVGJiokN5YmKiYmJiytzOgQMHZLPZ7Ovt2rXTd999p4KCAnvZt99+K5vNJldX12LbcHNzk7e3t8MCAACqLqdOgY0dO1aDBw9WVFSUoqOjtWzZMqWlpWnkyJGSrozMZGRkaNWqVZKu3CUWFhamiIgI5eXlafXq1dq8ebM2b95sb/Opp57S3/72N40ePVqjRo3SsWPHNGfOHP3pT39yyjkCAIDbj1MDUP/+/XXmzBnNmDFDmZmZat68uRISEuy3rWdmZiotLc1ePy8vT+PGjVNGRoY8PDwUERGhrVu3qkePHvY6wcHB2rFjh5577jm1aNFC9evX1+jRo/Xiiy9W+PkBAIDbk8UwDON6d0pPT5fFYlFQUJAkad++fVq7dq2aNWumESNGlHsnK1pOTo58fHyUnZ3NdBgAAJXE9Xx/39A1QAMGDLA/fycrK0v333+/9u3bp7/85S+aMWPGjTQJAABQYW4oAH399ddq3bq1JOntt99W8+bNlZKSorVr12rlypXl2T8AAIByd0MB6PLly3Jzc5Mk7dy5Uw888IAkqUmTJsrMzCy/3gEAANwCNxSAIiIi9Pe//13JyclKTExUt27dJEknT56Ur69vuXYQAACgvN1QAJo3b55ef/11derUSY8++qjuvvtuSVfew1U4NQYAAHC7uqG7wCQpPz9fOTk5ql27tr3sxIkT8vT0lL+/f7l10Bm4CwwAgMrnlt8F9ssvvyg3N9ceflJTUxUfH6+jR49W+vADAACqvhsKQL1797Y/nfncuXNq06aNFi5cqD59+mjp0qXl2kEAAIDydkMB6Msvv1T79u0lSZs2bVJAQIBSU1O1atUqvfLKK+XaQQAAgPJ2QwHo4sWL8vLykiTt2LFDffv2lYuLi9q2bavU1NRy7SAAAEB5u6EA1KhRI7377rtKT0/X9u3bFRcXJ0k6deoUFw0DAIDb3g0FoClTpmjcuHEKCwtT69atFR0dLenKaNA999xTrh0EAAAobzd8G3xWVpYyMzN19913y8XlSo7at2+fvL291aRJk3LtZEXjNngAACqf6/n+rnajBwkMDFRgYKB+/PFHWSwW1a9fn4cgAgCASuGGpsAKCgo0Y8YM+fj4KDQ0VCEhIapVq5ZmzpypgoKC8u4jAABAubqhEaCJEyfqzTff1F//+le1a9dOhmFo9+7dmjZtmi5duqTZs2eXdz8BAADKzQ1dA1SvXj39/e9/t78FvtB7772np59+WhkZGeXWQWfgGiAAACqfW/4qjLNnzxZ7oXOTJk109uzZG2kSAACgwtxQALr77rv16quvFil/9dVX1aJFi5vuFAAAwK10Q9cAvfTSS+rZs6d27typ6OhoWSwWpaSkKD09XQkJCeXdRwAAgHJ1QyNAHTt21LfffqsHH3xQ586d09mzZ9W3b1/97//+r1asWFHefQQAAChXN/wgxOIcPHhQrVq1Un5+fnk16RRcBA0AQOVzyy+CBgAAqMwIQAAAwHQIQAAAwHSu6y6wvn37lrr93LlzN9MXAACACnFdAcjHx+ea2x977LGb6hAAAMCtdl0BiFvcAQBAVcA1QAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHQIQAAAwHScHoCWLFmiBg0ayN3dXZGRkUpOTi6xblJSkiwWS5HlyJEjxdZfv369LBaL+vTpc4t6DwAAKiOnBqANGzZozJgxmjhxog4cOKD27dure/fuSktLK3W/o0ePKjMz077ceeedReqkpqZq3Lhxat++/a3qPgAAqKScGoAWLVqkYcOGafjw4WratKni4+MVHByspUuXlrqfv7+/AgMD7YvVanXYnp+fr4EDB2r69Olq2LDhrTwFAABQCTktAOXl5Wn//v2Ki4tzKI+Li1NKSkqp+95zzz2y2WyKjY3Vrl27imyfMWOG6tatq2HDhpWpL7m5ucrJyXFYAABA1eW0AHT69Gnl5+crICDAoTwgIEBZWVnF7mOz2bRs2TJt3rxZW7ZsUXh4uGJjY/Xpp5/a6+zevVtvvvmm3njjjTL3Ze7cufLx8bEvwcHBN3ZSAACgUqjm7A5YLBaHdcMwipQVCg8PV3h4uH09Ojpa6enpWrBggTp06KDz589r0KBBeuONN+Tn51fmPkyYMEFjx461r+fk5BCCAACowpwWgPz8/GS1WouM9pw6darIqFBp2rZtq9WrV0uSvv/+e504cUK9evWyby8oKJAkVatWTUePHtUdd9xRpA03Nze5ubndyGkAAIBKyGlTYK6uroqMjFRiYqJDeWJiomJiYsrczoEDB2Sz2SRJTZo00aFDh/TVV1/ZlwceeECdO3fWV199xagOAACQ5OQpsLFjx2rw4MGKiopSdHS0li1bprS0NI0cOVLSlampjIwMrVq1SpIUHx+vsLAwRUREKC8vT6tXr9bmzZu1efNmSZK7u7uaN2/ucIxatWpJUpFyAABgXk4NQP3799eZM2c0Y8YMZWZmqnnz5kpISFBoaKgkKTMz0+GZQHl5eRo3bpwyMjLk4eGhiIgIbd26VT169HDWKQAAgErIYhiG4exO3G5ycnLk4+Oj7OxseXt7O7s7AACgDK7n+9vpr8IAAACoaAQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOk4PQEuWLFGDBg3k7u6uyMhIJScnl1g3KSlJFoulyHLkyBF7nTfeeEPt27dX7dq1Vbt2bXXp0kX79u2riFMBAACVhFMD0IYNGzRmzBhNnDhRBw4cUPv27dW9e3elpaWVut/Ro0eVmZlpX+688077tqSkJD366KPatWuX9uzZo5CQEMXFxSkjI+NWnw4AAKgkLIZhGM46eJs2bdSqVSstXbrUXta0aVP16dNHc+fOLVI/KSlJnTt31n//+1/VqlWrTMfIz89X7dq19eqrr+qxxx4r0z45OTny8fFRdna2vL29y7QPAABwruv5/nbaCFBeXp7279+vuLg4h/K4uDilpKSUuu8999wjm82m2NhY7dq1q9S6Fy9e1OXLl1WnTp0S6+Tm5ionJ8dhAQAAVZfTAtDp06eVn5+vgIAAh/KAgABlZWUVu4/NZtOyZcu0efNmbdmyReHh4YqNjdWnn35a4nHGjx+v+vXrq0uXLiXWmTt3rnx8fOxLcHDwjZ0UAACoFKo5uwMWi8Vh3TCMImWFwsPDFR4ebl+Pjo5Wenq6FixYoA4dOhSp/9JLL2ndunVKSkqSu7t7iX2YMGGCxo4da1/PyckhBAEAUIU5bQTIz89PVqu1yGjPqVOniowKlaZt27Y6duxYkfIFCxZozpw52rFjh1q0aFFqG25ubvL29nZYAABA1eW0AOTq6qrIyEglJiY6lCcmJiomJqbM7Rw4cEA2m82hbP78+Zo5c6a2bdumqKiocukvAACoOpw6BTZ27FgNHjxYUVFRio6O1rJly5SWlqaRI0dKujI1lZGRoVWrVkmS4uPjFRYWpoiICOXl5Wn16tXavHmzNm/ebG/zpZde0uTJk7V27VqFhYXZR5hq1qypmjVrVvxJAgCA245TA1D//v115swZzZgxQ5mZmWrevLkSEhIUGhoqScrMzHR4JlBeXp7GjRunjIwMeXh4KCIiQlu3blWPHj3sdZYsWaK8vDz169fP4VhTp07VtGnTKuS8AADA7c2pzwG6XfEcIAAAKp9K8RwgAAAAZyEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA03HqqzAAoCLlF+QrOS1ZmeczZfOyqX1Ie1ldrM7uFgAnIAABMIUth7do9LbR+jHnR3tZkHeQXu72svo27evEngFwBqbAAFR5Ww5vUb+3+zmEH0nKyMlQv7f7acvhLU7qGQBnIQABqNLyC/I1ettoGSr63ufCsjHbxii/IL+iuwbAiQhAAKq05LTkIiM/v2XIUHpOupLTkiuwVwCcjQAEoErLPJ9ZrvUAVA0EIABVms3LVq71AFQNBCAAVVr7kPYK8g6SRZZit1tkUbB3sNqHtK/gngFwJgIQgCrN6mLVy91elqQiIahwPb5bPM8DAkyGAASgyuvbtK82PbxJ9b3rO5QHeQdp08ObeA4QYEIWwzCK3htqcjk5OfLx8VF2dra8vb2d3R0A5YQnQQNV2/V8f/MkaACmYXWxqlNYJ2d3A8BtgCkwAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOgQgAABgOjwJuhiFbwfJyclxck8AAEBZFX5vl+UtXwSgYpw/f16SFBwc7OSeAACA63X+/Hn5+PiUWoeXoRajoKBAJ0+elJeXlywWi7O7A6Ac5eTkKDg4WOnp6bzsGKhiDMPQ+fPnVa9ePbm4lH6VDwEIgKlcz9uiAVRdXAQNAABMhwAEAABMhwAEwFTc3Nw0depUubm5ObsrAJyIa4AAAIDpMAIEAABMhwAEAABMhwAEAABMhwAEAABMhwAEwFSWLFmiBg0ayN3dXZGRkUpOTnZ2lwA4AQEIgGls2LBBY8aM0cSJE3XgwAG1b99e3bt3V1pamrO7BqCCcRs8ANNo06aNWrVqpaVLl9rLmjZtqj59+mju3LlO7BmAisYIEABTyMvL0/79+xUXF+dQHhcXp5SUFCf1CoCzEIAAmMLp06eVn5+vgIAAh/KAgABlZWU5qVcAnIUABMBULBaLw7phGEXKAFR9BCAApuDn5yer1VpktOfUqVNFRoUAVH0EIACm4OrqqsjISCUmJjqUJyYmKiYmxkm9AuAs1ZzdAQCoKGPHjtXgwYMVFRWl6OhoLVu2TGlpaRo5cqSzuwagghGAAJhG//79debMGc2YMUOZmZlq3ry5EhISFBoa6uyuAahgPAcIAACYDtcAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAQAA0yEAAUAZWCwWvfvuu87uBoByQgACcNsbOnSoLBZLkaVbt27O7hqASopXYQCoFLp166YVK1Y4lLm5uTmpNwAqO0aAAFQKbm5uCgwMdFhq164t6cr01NKlS9W9e3d5eHioQYMG2rhxo8P+hw4d0n333ScPDw/5+vpqxIgRunDhgkOd5cuXKyIiQm5ubrLZbHr22Wcdtp8+fVoPPvigPD09deedd+r999+/tScN4JYhAAGoEiZPnqyHHnpIBw8e1KBBg/Too4/q8OHDkqSLFy+qW7duql27tj7//HNt3LhRO3fudAg4S5cu1TPPPKMRI0bo0KFDev/999WoUSOHY0yfPl0PP/yw/v3vf6tHjx4aOHCgzp49W6HnCaCcGABwmxsyZIhhtVqNGjVqOCwzZswwDMMwJBkjR4502KdNmzbGU089ZRiGYSxbtsyoXbu2ceHCBfv2rVu3Gi4uLkZWVpZhGIZRr149Y+LEiSX2QZIxadIk+/qFCxcMi8VifPjhh+V2ngAqDtcAAagUOnfurKVLlzqU1alTx/5zdHS0w7bo6Gh99dVXkqTDhw/r7rvvVo0aNezb27Vrp4KCAh09elQWi0UnT55UbGxsqX1o0aKF/ecaNWrIy8tLp06dutFTAuBEBCAAlUKNGjWKTEldi8VikSQZhmH/ubg6Hh4eZWqvevXqRfYtKCi4rj4BuD1wDRCAKmHv3r1F1ps0aSJJatasmb766iv9/PPP9u27d++Wi4uLGjduLC8vL4WFhemjjz6q0D4DcB5GgABUCrm5ucrKynIoq1atmvz8/CRJGzduVFRUlO69916tWbNG+/bt05tvvilJGjhwoKZOnaohQ4Zo2rRp+umnnzRq1CgNHjxYAQEBkqRp06Zp5MiR8vf3V/fu3XX+/Hnt3r1bo0aNqtgTBVAhCEAAKoVt27bJZrM5lIWHh+vIkSOSrtyhtX79ej399NMKDAzUmjVr1KxZM0mSp6entm/frtGjR+t3v/udPD099dBDD2nRokX2toYMGaJLly5p8eLFGjdunPz8/NSvX7+KO0EAFcpiGIbh7E4AwM2wWCx655131KdPH2d3BUAlwTVAAADAdAhAAADAdLgGCEClx0w+gOvFCBAAADAdAhAAADAdAhAAADAdAhAAADAdAhAAADAdAhAAADAdAhAAADAdAhAAADAdAhAAADCd/weU/riUYlevBQAAAABJRU5ErkJggg=="},"metadata":{}}]},{"cell_type":"markdown","source":"# Performance On Test Set\n## Data Preparation","metadata":{}},{"cell_type":"code","source":"test = convert_to_dataset_torch(X_test, y_test)\ntest_dataloader = DataLoader(test,  sampler=SequentialSampler(test), batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:29:18.901509Z","iopub.execute_input":"2023-06-30T17:29:18.902576Z","iopub.status.idle":"2023-06-30T17:29:19.414061Z","shell.execute_reply.started":"2023-06-30T17:29:18.902535Z","shell.execute_reply":"2023-06-30T17:29:19.410922Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"  0%|          | 0/600 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n100%|██████████| 600/600 [00:00<00:00, 1215.98it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluate on Test Set\nWith the test set prepared, we can apply our fine-tuned model to generate predictions on the test set:","metadata":{}},{"cell_type":"code","source":"bert_model.eval()\n\n_, _,_ ,predicted_labels = eval_batch(test_dataloader, bert_model)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:29:19.415969Z","iopub.execute_input":"2023-06-30T17:29:19.41636Z","iopub.status.idle":"2023-06-30T17:34:13.047701Z","shell.execute_reply.started":"2023-06-30T17:29:19.416326Z","shell.execute_reply":"2023-06-30T17:34:13.045473Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"Evaluating: 100%|██████████| 30/30 [04:53<00:00,  9.79s/batch]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Let's plot confusion matix over the test results:","metadata":{}},{"cell_type":"code","source":"!pip install ds_utils","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:34:13.051451Z","iopub.execute_input":"2023-06-30T17:34:13.052516Z","iopub.status.idle":"2023-06-30T17:34:32.921938Z","shell.execute_reply.started":"2023-06-30T17:34:13.052462Z","shell.execute_reply":"2023-06-30T17:34:32.920838Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting ds_utils\n  Downloading ds_utils-0.4.1.tar.gz (2.1 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting boto (from ds_utils)\n  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ds_utils\n  Building wheel for ds_utils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ds_utils: filename=ds_utils-0.4.1-py3-none-any.whl size=2659 sha256=0f01ffc598575b4ddc8c3f8814e0fe4b6b277a056c6e47f6a5490065cfe8e366\n  Stored in directory: /root/.cache/pip/wheels/49/58/05/627bdfaf592214bc7588a119920d4499598d147b4e1f691513\nSuccessfully built ds_utils\nInstalling collected packages: boto, ds_utils\nSuccessfully installed boto-2.49.0 ds_utils-0.4.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install ds_utils\nfrom ds_utils.metrics import plot_confusion_matrix\n\n\nplot_confusion_matrix(y_test, predicted_labels, [1, 0])\npyplot.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:34:32.924054Z","iopub.execute_input":"2023-06-30T17:34:32.924554Z","iopub.status.idle":"2023-06-30T17:34:34.104741Z","shell.execute_reply.started":"2023-06-30T17:34:32.924491Z","shell.execute_reply":"2023-06-30T17:34:34.102512Z"},"trusted":true},"execution_count":29,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# !pip install ds_utils\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mds_utils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n\u001b[1;32m      5\u001b[0m plot_confusion_matrix(y_test, predicted_labels, [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      6\u001b[0m pyplot\u001b[38;5;241m.\u001b[39mshow()\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ds_utils'"],"ename":"ModuleNotFoundError","evalue":"No module named 'ds_utils'","output_type":"error"}]},{"cell_type":"markdown","source":"# Saving the Fine-Tuned Model\nFirst let's save our model with the method ``save_pretrained``, then save our tokenizer with ``save_pretrained``.\n\n**Pay Attension**: tokenizer's ``save_pretrained`` need to get the path to save as a string.","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\n\n\n\noutput_dir = Path(\"__file__\").parents[0].absolute().joinpath(\"model_save\")\noutput_dir.mkdir(exist_ok=True)\n\n# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n# They can then be reloaded using `from_pretrained()`\nmodel_to_save = bert_model.module if hasattr(bert_model, 'module') else bert_model  # Take care of distributed/parallel training\nmodel_to_save.save_pretrained(output_dir)\ntokenizer.save_pretrained(str(output_dir.absolute()))","metadata":{"execution":{"iopub.status.busy":"2023-06-30T17:34:34.106092Z","iopub.status.idle":"2023-06-30T17:34:34.106565Z","shell.execute_reply.started":"2023-06-30T17:34:34.106349Z","shell.execute_reply":"2023-06-30T17:34:34.106371Z"},"trusted":true},"execution_count":null,"outputs":[]}]}